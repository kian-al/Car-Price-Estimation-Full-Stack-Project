import pandas as pd
import joblib
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import os
import time
import numpy as np

# ========================================
# ðŸŽ¯ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# ========================================

SAMPLE_SIZE = 200_000  # ðŸ‘ˆ ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ
USE_SAMPLING = True     # ðŸ‘ˆ True = Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡ | False = Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ù‡

# ========================================
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# ========================================

def persian_to_english_numbers(text):
    """ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ"""
    if pd.isna(text):
        return text
    text = str(text)
    persian_digits = 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹'
    english_digits = '0123456789'
    translation = str.maketrans(persian_digits, english_digits)
    text = text.replace('Ù¬', '').replace(',', '').replace(' ', '')
    return text.translate(translation)

def extract_brand(brand_text):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø±Ù†Ø¯ Ø§ØµÙ„ÛŒ"""
    if pd.isna(brand_text):
        return 'Ù†Ø§Ù…Ø´Ø®Øµ'
    brand_text = str(brand_text)
    brand = brand_text.split('ØŒ')[0].split(',')[0].strip()
    return brand if brand else 'Ù†Ø§Ù…Ø´Ø®Øµ'

# ========================================
# Ø´Ø±ÙˆØ¹ Ø¨Ø±Ù†Ø§Ù…Ù‡
# ========================================

print("="*70)
print("ðŸš— Car Price Prediction Model - Training System")
print("="*70)
print(f"ðŸ“Š Configuration:")
print(f"   Sample size: {SAMPLE_SIZE:,} rows")
print(f"   Use sampling: {USE_SAMPLING}")
print("="*70)

start_time = time.time()

# ========================================
# ðŸ”¹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡
# ========================================

print("\nðŸ“¥ Loading dataset...")

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CSV_PATH = os.path.join(BASE_DIR, 'final2024.csv')

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
if not os.path.exists(CSV_PATH):
    print(f"âŒ File not found: {CSV_PATH}")
    print(f"ðŸ’¡ Expected location: {os.path.abspath(CSV_PATH)}")
    print("ðŸ’¡ Make sure 'final2024.csv' is in the correct folder")
    exit(1)

df = pd.read_csv(CSV_PATH, low_memory=False)

print(f"âœ… Dataset loaded successfully!")
print(f"   Total rows: {len(df):,}")
print(f"   Total columns: {len(df.columns)}")
print(f"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ========================================
# ðŸ”¹ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
# ========================================

print("\n" + "="*70)
print("ðŸ§¹ Data Cleaning...")
print("="*70)

# 1. ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ
print("\n1ï¸âƒ£ Converting Persian numbers to English...")
for col in ['Model_Year', 'Mileage', 'Price']:
    if col in df.columns:
        df[col] = df[col].apply(persian_to_english_numbers)
print("   âœ… Conversion completed")

# 2. ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ
print("\n2ï¸âƒ£ Converting to numeric types...")
df['Price'] = pd.to_numeric(df['Price'], errors='coerce')
df['Model_Year'] = pd.to_numeric(df['Model_Year'], errors='coerce')
df['Mileage'] = pd.to_numeric(df['Mileage'], errors='coerce')

# 3. ÙÛŒÙ„ØªØ± Ù‚ÛŒÙ…Øª Ù…Ø¹ØªØ¨Ø±
print("\n3ï¸âƒ£ Filtering valid prices...")
initial_count = len(df)
df = df[df['Price'].notna()]
df = df[(df['Price'] > 10_000_000) & (df['Price'] < 10_000_000_000)]
print(f"   Valid prices: {len(df):,} / {initial_count:,} ({len(df)/initial_count*100:.1f}%)")

# 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø±Ù†Ø¯
print("\n4ï¸âƒ£ Extracting main brand...")
df['Brand_Main'] = df['Brand'].apply(extract_brand)
unique_brands = df['Brand_Main'].nunique()
top_brands = df['Brand_Main'].value_counts().head(5)
print(f"   Unique brands: {unique_brands}")
print(f"   Top 5 brands:")
for brand, count in top_brands.items():
    print(f"      - {brand}: {count:,} ({count/len(df)*100:.1f}%)")

# 5. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ù† Ø®ÙˆØ¯Ø±Ùˆ
print("\n5ï¸âƒ£ Calculating car age...")
current_year = 1403
df['Car_Age'] = current_year - df['Model_Year']
df = df[(df['Car_Age'] >= 0) & (df['Car_Age'] <= 50)]
print(f"   Age range: {df['Car_Age'].min():.0f} - {df['Car_Age'].max():.0f} years")
print(f"   Average age: {df['Car_Age'].mean():.1f} years")

# 6. Ù…Ø¯ÛŒØ±ÛŒØª Ú©ÛŒÙ„ÙˆÙ…ØªØ±
print("\n6ï¸âƒ£ Handling mileage...")
mileage_median = df['Mileage'].median()
outliers_count = len(df[(df['Mileage'] < 0) | (df['Mileage'] > 1_000_000)])
df.loc[df['Mileage'] < 0, 'Mileage'] = mileage_median
df.loc[df['Mileage'] > 1_000_000, 'Mileage'] = mileage_median
print(f"   Mileage median: {mileage_median:,.0f} km")
print(f"   Fixed outliers: {outliers_count:,}")

print(f"\nâœ… Cleaned dataset: {len(df):,} rows")

# ========================================
# ðŸ”¹ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ
# ========================================

print("\n" + "="*70)
print("ðŸ“Š Sampling Strategy...")
print("="*70)

if USE_SAMPLING and len(df) > SAMPLE_SIZE:
    print(f"\nâš¡ Sampling {SAMPLE_SIZE:,} rows from {len(df):,} total rows")
    df_sample = df.sample(n=SAMPLE_SIZE, random_state=42)
    print(f"   Sample ratio: {SAMPLE_SIZE/len(df)*100:.1f}%")
    print(f"   Remaining data: {len(df) - SAMPLE_SIZE:,} rows not used")
elif not USE_SAMPLING:
    print(f"\nðŸ“¦ Using ALL {len(df):,} rows (no sampling)")
    df_sample = df
else:
    print(f"\nâš ï¸ Dataset has only {len(df):,} rows (less than {SAMPLE_SIZE:,})")
    print(f"   Using all available data")
    df_sample = df

print(f"âœ… Final data for training: {len(df_sample):,} rows")

# ========================================
# ðŸ”¹ Ø§Ù†ØªØ®Ø§Ø¨ Feature Ù‡Ø§
# ========================================

print("\n" + "="*70)
print("ðŸŽ¯ Feature Engineering...")
print("="*70)

features = [
    "Brand_Main",
    "Car_Age",
    "Mileage",
    "City",
    "Gearbox",
    "Fuel_Type",
    "Body_Condition",
    "Engine_Condition",
    "Chassis_Condition",
]

print(f"\nSelected features ({len(features)}):")
for i, feat in enumerate(features, 1):
    print(f"   {i}. {feat}")

# Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡ Ø§ØµÙ„ÛŒ
df_clean = df_sample.dropna(subset=['Brand_Main', 'Car_Age', 'Mileage'], how='all')
dropped = len(df_sample) - len(df_clean)

X = df_clean[features]
y = df_clean['Price']

print(f"\nData after feature selection:")
print(f"   Valid rows: {len(df_clean):,}")
print(f"   Dropped rows: {dropped:,}")
print(f"   Feature matrix shape: {X.shape}")

# ========================================
# ðŸ”¹ Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡
# ========================================

print("\n" + "="*70)
print("ðŸ“Š Dataset Statistics:")
print("="*70)

print(f"\nðŸ’° Price Distribution:")
print(f"   Min:       {y.min():>15,.0f} ØªÙˆÙ…Ø§Ù† ({y.min()/1_000_000:.1f}M)")
print(f"   Max:       {y.max():>15,.0f} ØªÙˆÙ…Ø§Ù† ({y.max()/1_000_000:.1f}M)")
print(f"   Mean:      {y.mean():>15,.0f} ØªÙˆÙ…Ø§Ù† ({y.mean()/1_000_000:.1f}M)")
print(f"   Median:    {y.median():>15,.0f} ØªÙˆÙ…Ø§Ù† ({y.median()/1_000_000:.1f}M)")
print(f"   Std Dev:   {y.std():>15,.0f} ØªÙˆÙ…Ø§Ù† ({y.std()/1_000_000:.1f}M)")

print(f"\nðŸš— Car Age Distribution:")
print(f"   Min:       {df_clean['Car_Age'].min():>6.0f} years")
print(f"   Max:       {df_clean['Car_Age'].max():>6.0f} years")
print(f"   Mean:      {df_clean['Car_Age'].mean():>6.1f} years")
print(f"   Median:    {df_clean['Car_Age'].median():>6.0f} years")

print(f"\nðŸ“ Mileage Distribution:")
print(f"   Min:       {df_clean['Mileage'].min():>10,.0f} km")
print(f"   Max:       {df_clean['Mileage'].max():>10,.0f} km")
print(f"   Mean:      {df_clean['Mileage'].mean():>10,.0f} km")
print(f"   Median:    {df_clean['Mileage'].median():>10,.0f} km")

# ========================================
# ðŸ”¹ Preprocessing Pipeline
# ========================================

print("\n" + "="*70)
print("âš™ï¸ Building Preprocessing Pipeline...")
print("="*70)

numeric_features = ['Car_Age', 'Mileage']
categorical_features = [f for f in features if f not in numeric_features]

print(f"\nðŸ“Š Feature types:")
print(f"   Numeric ({len(numeric_features)}): {numeric_features}")
print(f"   Categorical ({len(categorical_features)}): {categorical_features}")

preprocessor = ColumnTransformer(
    transformers=[
        ('num', SimpleImputer(strategy='median'), numeric_features),
        ('cat', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='Ù†Ø§Ù…Ø´Ø®Øµ')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ]), categorical_features)
    ]
)

# ========================================
# ðŸ”¹ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„
# ========================================

print("\n" + "="*70)
print("ðŸ§  Model Configuration:")
print("="*70)

model = RandomForestRegressor(
    n_estimators=150,      # ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®Øªâ€ŒÙ‡Ø§
    max_depth=18,          # Ø¹Ù…Ù‚ Ù‡Ø± Ø¯Ø±Ø®Øª
    min_samples_split=8,   # Ø­Ø¯Ø§Ù‚Ù„ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ…
    min_samples_leaf=4,    # Ø­Ø¯Ø§Ù‚Ù„ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± Ø¨Ø±Ú¯
    max_features='sqrt',   # ØªØ¹Ø¯Ø§Ø¯ feature Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªÙ‚Ø³ÛŒÙ…
    random_state=42,
    n_jobs=-1,             # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ù‡ Ù‡Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ CPU
    verbose=1              # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´Ø±ÙØª
)

print(f"\nModel hyperparameters:")
print(f"   n_estimators:      {model.n_estimators}")
print(f"   max_depth:         {model.max_depth}")
print(f"   min_samples_split: {model.min_samples_split}")
print(f"   min_samples_leaf:  {model.min_samples_leaf}")
print(f"   max_features:      {model.max_features}")
print(f"   n_jobs:            {model.n_jobs} (all available cores)")

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])

# ========================================
# ðŸ”¹ ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡
# ========================================

print("\n" + "="*70)
print("ðŸ”€ Splitting Dataset...")
print("="*70)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nðŸ“¦ Data split:")
print(f"   Training set:   {X_train.shape[0]:>8,} samples ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"   Test set:       {X_test.shape[0]:>8,} samples ({X_test.shape[0]/len(X)*100:.1f}%)")
print(f"   Train/Test:     {X_train.shape[0]/X_test.shape[0]:.1f}:1")

# ========================================
# ðŸ”¹ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
# ========================================

print("\n" + "="*70)
print("ðŸš€ TRAINING MODEL...")
print("="*70)
print(f"\nâ³ Expected time: 10-15 minutes for {len(X_train):,} samples")
print(f"ðŸ’¡ Progress bars will appear below...")
print(f"â˜• Time for a coffee break!\n")

train_start = time.time()
pipeline.fit(X_train, y_train)
train_time = time.time() - train_start

print(f"\nâœ… Training completed!")
print(f"â±ï¸ Training time: {train_time:.1f} seconds ({train_time/60:.2f} minutes)")
print(f"âš¡ Speed: {len(X_train)/train_time:.0f} samples/second")

# ========================================
# ðŸ”¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
# ========================================

print("\n" + "="*70)
print("ðŸ“Š Evaluating Model Performance...")
print("="*70)

print("\nðŸ”® Making predictions...")
eval_start = time.time()

train_preds = pipeline.predict(X_train)
test_preds = pipeline.predict(X_test)

eval_time = time.time() - eval_start
print(f"   Prediction time: {eval_time:.2f} seconds")

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
train_mae = mean_absolute_error(y_train, train_preds)
test_mae = mean_absolute_error(y_test, test_preds)

train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))
test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))

train_r2 = r2_score(y_train, train_preds)
test_r2 = r2_score(y_test, test_preds)

train_mape = np.mean(np.abs((y_train - train_preds) / y_train)) * 100
test_mape = np.mean(np.abs((y_test - test_preds) / y_test)) * 100

# ========================================
# ðŸ”¹ Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
# ========================================

print("\n" + "="*70)
print("ðŸ“Š FINAL MODEL PERFORMANCE")
print("="*70)

print(f"\nðŸŽ¯ Training Set:")
print(f"   MAE:     {train_mae:>15,.0f} ØªÙˆÙ…Ø§Ù†  ({train_mae/1_000_000:>6.2f} M)")
print(f"   RMSE:    {train_rmse:>15,.0f} ØªÙˆÙ…Ø§Ù†  ({train_rmse/1_000_000:>6.2f} M)")
print(f"   RÂ²:      {train_r2:>15.4f}          ({train_r2*100:>6.2f}%)")
print(f"   MAPE:    {train_mape:>15.2f}%")

print(f"\nðŸŽ¯ Test Set:")
print(f"   MAE:     {test_mae:>15,.0f} ØªÙˆÙ…Ø§Ù†  ({test_mae/1_000_000:>6.2f} M)")
print(f"   RMSE:    {test_rmse:>15,.0f} ØªÙˆÙ…Ø§Ù†  ({test_rmse/1_000_000:>6.2f} M)")
print(f"   RÂ²:      {test_r2:>15.4f}          ({test_r2*100:>6.2f}%)")
print(f"   MAPE:    {test_mape:>15.2f}%")

# Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª
print(f"\nðŸ’¡ Model Quality:")
if test_r2 > 0.90:
    quality = "ðŸŸ¢ EXCELLENT - Outstanding performance!"
elif test_r2 > 0.85:
    quality = "ðŸŸ¢ VERY GOOD - Great results!"
elif test_r2 > 0.80:
    quality = "ðŸŸ¡ GOOD - Solid performance"
elif test_r2 > 0.70:
    quality = "ðŸŸ¡ ACCEPTABLE - Usable but could improve"
else:
    quality = "ðŸ”´ NEEDS IMPROVEMENT - Consider more data/features"

print(f"   {quality}")
print(f"   Model explains {test_r2*100:.1f}% of price variance")
print(f"   Average prediction error: Â±{test_mape:.1f}%")

# Ø¨Ø±Ø±Ø³ÛŒ Overfitting
overfit_score = train_r2 - test_r2
print(f"\nðŸ“ˆ Overfitting Check:")
if overfit_score < 0.05:
    print(f"   âœ… Excellent - Low overfitting (Î”={overfit_score:.3f})")
elif overfit_score < 0.10:
    print(f"   âš ï¸ Moderate overfitting (Î”={overfit_score:.3f})")
else:
    print(f"   âŒ High overfitting detected (Î”={overfit_score:.3f})")
    print(f"      Consider: reducing max_depth or n_estimators")

# ========================================
# ðŸ”¹ Ù†Ù…ÙˆÙ†Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
# ========================================

print("\n" + "="*70)
print("ðŸ”® Sample Predictions:")
print("="*70)

sample_size = min(15, len(y_test))
sample_results = pd.DataFrame({
    'Actual': y_test.head(sample_size).values,
    'Predicted': test_preds[:sample_size],
    'Error': np.abs(y_test.head(sample_size).values - test_preds[:sample_size]),
    'Error%': np.abs(y_test.head(sample_size).values - test_preds[:sample_size]) / y_test.head(sample_size).values * 100
})

# ÙØ±Ù…Øª
sample_results['Actual_Fmt'] = sample_results['Actual'].apply(lambda x: f"{x:>12,.0f}")
sample_results['Predicted_Fmt'] = sample_results['Predicted'].apply(lambda x: f"{x:>12,.0f}")
sample_results['Error_Fmt'] = sample_results['Error'].apply(lambda x: f"{x:>12,.0f}")
sample_results['Error%_Fmt'] = sample_results['Error%'].apply(lambda x: f"{x:>6.1f}%")

print(f"\n{'Actual':>15s} {'Predicted':>15s} {'Error':>15s} {'Error%':>10s}")
print("-" * 60)
for _, row in sample_results.iterrows():
    print(f"{row['Actual_Fmt']} {row['Predicted_Fmt']} {row['Error_Fmt']} {row['Error%_Fmt']}")

# ========================================
# ðŸ”¹ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
# ========================================

print("\n" + "="*70)
print("ðŸ’¾ Saving Model...")
print("="*70)

# Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„
MODEL_PATH = os.path.join(BASE_DIR, 'car_price_model.pkl')
joblib.dump(pipeline, MODEL_PATH)
model_size = os.path.getsize(MODEL_PATH) / (1024 * 1024)
print(f"\nâœ… Model saved: {MODEL_PATH}")
print(f"   File size: {model_size:.2f} MB")

# Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
model_info = {
    'features': features,
    'numeric_features': numeric_features,
    'categorical_features': categorical_features,
    'sample_size': len(df_clean),
    'train_samples': len(X_train),
    'test_samples': len(X_test),
    'train_time_seconds': train_time,
    'train_mae': train_mae,
    'test_mae': test_mae,
    'train_rmse': train_rmse,
    'test_rmse': test_rmse,
    'train_r2': train_r2,
    'test_r2': test_r2,
    'train_mape': train_mape,
    'test_mape': test_mape,
    'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),
    'config': {
        'n_estimators': model.n_estimators,
        'max_depth': model.max_depth,
        'sample_size_used': SAMPLE_SIZE
    }
}

INFO_PATH = os.path.join(BASE_DIR, 'model_features.pkl')
joblib.dump(model_info, INFO_PATH)
print(f"âœ… Model info saved: {INFO_PATH}")

# ========================================
# ðŸ”¹ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
# ========================================

total_time = time.time() - start_time

print("\n" + "="*70)
print("âœ… TRAINING COMPLETED SUCCESSFULLY!")
print("="*70)

print(f"\nðŸ“Š Summary:")
print(f"   Dataset size:        {len(df):,} rows")
print(f"   Samples used:        {len(df_clean):,} rows")
print(f"   Training samples:    {len(X_train):,}")
print(f"   Test samples:        {len(X_test):,}")
print(f"   Training time:       {train_time/60:.2f} minutes")
print(f"   Total execution:     {total_time/60:.2f} minutes")

print(f"\nðŸŽ¯ Performance:")
print(f"   Test MAE:            {test_mae/1_000_000:.2f} million Toman")
print(f"   Test RÂ²:             {test_r2:.4f} ({test_r2*100:.1f}%)")
print(f"   Test MAPE:           {test_mape:.2f}%")
print(f"   Model quality:       {quality}")

print(f"\nðŸ“¦ Output files:")
print(f"   1. car_price_model.pkl ({model_size:.2f} MB)")
print(f"   2. model_features.pkl")

print(f"\nðŸš€ Next steps:")
print(f"   - Run 'test_model.py' to test predictions")
print(f"   - Use the model in your application")
print(f"   - Share the .pkl files for deployment")

print("\n" + "="*70)
print("âœ… Ready to make predictions!")
print("="*70)